{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMbRhvSqc-yU",
        "outputId": "2bb8e96b-46d3-401e-dd40-452850573fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate Columns: Index([], dtype='object')\n",
            "Low Variability Columns: []\n",
            "Highly Correlated Pairs: []\n",
            "Columns with High Missing Values: Index(['Unnamed: 6', 'closed_at'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('cleaned_data.xlsx')\n",
        "\n",
        "# Step 1: Check for duplicates\n",
        "duplicate_columns = df.columns[df.columns.duplicated()]\n",
        "\n",
        "# Step 2: Low variability columns\n",
        "low_variability_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "\n",
        "# Step 3: Correlation check for numeric columns only\n",
        "numeric_df = df.select_dtypes(include=['number'])  # Select only numeric columns\n",
        "correlation_matrix = numeric_df.corr()\n",
        "high_corr_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns\n",
        "                   if i != j and abs(correlation_matrix.loc[i, j]) > 0.9]\n",
        "\n",
        "# Step 4: Identify categorical and numeric columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "numeric_cols = numeric_df.columns  # Only numeric columns\n",
        "\n",
        "# Step 5: Columns with high missing values\n",
        "missing_values = df.isnull().sum()\n",
        "high_missing_cols = missing_values[missing_values > (0.5 * len(df))].index  # Columns with >50% missing\n",
        "\n",
        "# Summary of potential columns to drop\n",
        "print(\"Duplicate Columns:\", duplicate_columns)\n",
        "print(\"Low Variability Columns:\", low_variability_cols)\n",
        "print(\"Highly Correlated Pairs:\", high_corr_pairs)\n",
        "print(\"Columns with High Missing Values:\", high_missing_cols)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping columns with high missing values\n",
        "df_cleaned = df.drop(columns=['Unnamed: 6', 'closed_at'])\n",
        "\n",
        "# Save the cleaned dataset if needed\n",
        "df_cleaned.to_excel('cleaned_data_updated.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "VKkAXIxDeA4O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_excel('cleaned_data_updated.xlsx')\n",
        "\n",
        "# Step 1: Preprocess the dataset\n",
        "# Drop any columns that are still irrelevant (like IDs or highly missing columns if present)\n",
        "df = df.drop(columns=['object_id'], errors='ignore')  # Replace 'object_id' with any ID-like column\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'status'  # Replace with the actual target column name\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Step 2: Convert categorical columns to numeric with one-hot encoding\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Initialize and train the Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNFf1gxHeO_V",
        "outputId": "92bf3245-b086-4212-92f6-09285f78c093"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9458483754512635\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    acquired       0.92      1.00      0.96       177\n",
            "      closed       1.00      0.85      0.92       100\n",
            "\n",
            "    accuracy                           0.95       277\n",
            "   macro avg       0.96      0.93      0.94       277\n",
            "weighted avg       0.95      0.95      0.94       277\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[177   0]\n",
            " [ 15  85]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the cleaned dataset\n",
        "df = pd.read_excel('cleaned_data_updated.xlsx')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=['object_id'], errors='ignore')  # Replace 'object_id' with any ID-like column\n",
        "\n",
        "# Define the target variable and features\n",
        "target = 'status'  # Replace with the actual target column name\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Convert categorical columns to numeric with one-hot encoding\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Impute missing values in X\n",
        "imputer = SimpleImputer(strategy='mean')  # Use 'mean' for numerical data, change as needed\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_rep)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_mat)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHT5tydFe6hV",
        "outputId": "fde1ec00-2f02-4e05-af9f-07edfbbfed99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7220216606498195\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    acquired       0.73      0.89      0.80       177\n",
            "      closed       0.68      0.43      0.53       100\n",
            "\n",
            "    accuracy                           0.72       277\n",
            "   macro avg       0.71      0.66      0.67       277\n",
            "weighted avg       0.72      0.72      0.70       277\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[157  20]\n",
            " [ 57  43]]\n"
          ]
        }
      ]
    }
  ]
}